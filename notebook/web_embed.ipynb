{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from numpy.linalg import norm\n",
        "\n",
        "\n",
        "def cosine(a, b):\n",
        "    return np.dot(a, b) / (norm(a) * norm(b))\n",
        "\n",
        "\n",
        "def split_sentences(text):\n",
        "    text = text.strip()\n",
        "    # g·ªôp nhi·ªÅu newline li√™n ti·∫øp th√†nh 1\n",
        "    text = re.sub(r'\\n+', '\\n', text)\n",
        "\n",
        "    return [\n",
        "        s.strip()\n",
        "        for s in re.split(r'(?<=[.!?])\\s+|\\n', text)\n",
        "        if s.strip()\n",
        "    ]\n",
        "\n",
        "device=\"cuda\"\n",
        "model = SentenceTransformer(\"BAAI/bge-m3\", device=device)\n",
        "threshold = 0.40\n"
      ],
      "metadata": {
        "id": "rc1QlP2EJTAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_sliding_window(\n",
        "    sentences,\n",
        "    window_size=5,\n",
        "    stride=2\n",
        "):\n",
        "    chunks = []\n",
        "    n = len(sentences)\n",
        "\n",
        "    for start in range(0, n, stride):\n",
        "        window = sentences[start:start + window_size]\n",
        "        if len(window) < 2:\n",
        "            break\n",
        "        chunks.append(\" \".join(window))\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "dJvi4G82bcpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('test_web.json', 'r') as f:\n",
        "    val_web = json.load(f)\n",
        "\n",
        "val_web_pages = []\n",
        "for val_web_query in val_web:\n",
        "    val_web_page = {}\n",
        "    val_web_page['question'] = val_web_query['question']\n",
        "    val_web_page['pages'] = val_web_query['info'].split('\\n\\n')\n",
        "    val_web_pages.append(val_web_page)"
      ],
      "metadata": {
        "id": "Zdpne_A3a_WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def cosine(a, b):\n",
        "    return np.dot(a, b)\n",
        "\n",
        "val_selected_chunks = []\n",
        "\n",
        "# ====== STAT ======\n",
        "total_chunks = 0\n",
        "selected_chunks_cnt = 0\n",
        "# ==================\n",
        "\n",
        "for val_web_page in tqdm(val_web_pages, desc=\"Web pages\"):\n",
        "    query = val_web_page['question']\n",
        "    pages = val_web_page['pages']\n",
        "\n",
        "    query_embedding = model.encode(\n",
        "        query,\n",
        "        normalize_embeddings=True\n",
        "    )\n",
        "\n",
        "    val_selected_chunk = {\n",
        "        'question': query,\n",
        "        'chunks_page': []\n",
        "    }\n",
        "\n",
        "    for page in pages:\n",
        "        sentences = split_sentences(page)\n",
        "        chunks = chunk_sliding_window(\n",
        "            sentences,\n",
        "            window_size=5,\n",
        "            stride=2\n",
        "        )\n",
        "\n",
        "        total_chunks += len(chunks)\n",
        "\n",
        "        if not chunks:\n",
        "            val_selected_chunk['chunks_page'].append([])\n",
        "            continue\n",
        "\n",
        "        chunk_embeddings = model.encode(\n",
        "            chunks,\n",
        "            normalize_embeddings=True\n",
        "        )\n",
        "\n",
        "        sims = np.dot(chunk_embeddings, query_embedding)\n",
        "\n",
        "        selected_chunks = []\n",
        "        for chunk, sim in zip(chunks, sims):\n",
        "            if sim > threshold:\n",
        "                selected_chunks.append(chunk)\n",
        "                selected_chunks_cnt += 1\n",
        "\n",
        "        val_selected_chunk['chunks_page'].append(selected_chunks)\n",
        "\n",
        "    val_selected_chunks.append(val_selected_chunk)\n",
        "\n"
      ],
      "metadata": {
        "id": "feB-1iw1azua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(selected_chunks_cnt, total_chunks)"
      ],
      "metadata": {
        "id": "H94bH-HCkXuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"test_selected_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(\n",
        "        val_selected_chunks,\n",
        "        f,\n",
        "        ensure_ascii=False,\n",
        "        indent=2\n",
        "    )"
      ],
      "metadata": {
        "collapsed": true,
        "id": "K3zuYHGcfVCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "VjvLZeNsw3xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 11eVbxCq8N-m2VrK5l2wLQNHetf8vCIoK"
      ],
      "metadata": {
        "id": "ylEugb_01vys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!du -sh ./*\n"
      ],
      "metadata": {
        "id": "TNdoi68u2C16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "\n",
        "def safe_normalize(embeddings, tol=1e-3):\n",
        "    \"\"\"\n",
        "    embeddings: np.ndarray (N, d)\n",
        "    tol: sai s·ªë cho ph√©p so v·ªõi norm = 1\n",
        "    \"\"\"\n",
        "    norms = np.linalg.norm(embeddings, axis=1)\n",
        "\n",
        "    # Ki·ªÉm tra: c√≥ vector n√†o ch∆∞a norm kh√¥ng?\n",
        "    need_norm = np.any(np.abs(norms - 1.0) > tol)\n",
        "\n",
        "    if need_norm:\n",
        "        faiss.normalize_L2(embeddings)\n",
        "        print(\"üîÑ Embeddings were NOT normalized ‚Üí normalized now.\")\n",
        "    else:\n",
        "        print(\"‚úÖ Embeddings already normalized ‚Üí skip.\")\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "def save_index_streaming(\n",
        "    input_json_path,\n",
        "    index_out_path,\n",
        "    batch_size=4096,\n",
        "):\n",
        "    with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    if isinstance(data, dict):\n",
        "        data = [data]\n",
        "\n",
        "    dim = len(data[0][\"embeddings\"])\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "\n",
        "    buf = []\n",
        "\n",
        "    for i, item in enumerate(data):\n",
        "        buf.append(item[\"embeddings\"])\n",
        "\n",
        "        if len(buf) == batch_size:\n",
        "            batch = np.array(buf, dtype=\"float32\")\n",
        "            index.add(batch)\n",
        "            buf.clear()\n",
        "\n",
        "            if i % (batch_size * 10) == 0:\n",
        "                print(f\"Added {index.ntotal} vectors\")\n",
        "\n",
        "    # add ph·∫ßn c√≤n l·∫°i\n",
        "    if buf:\n",
        "        batch = np.array(buf, dtype=\"float32\")\n",
        "        index.add(batch)\n",
        "\n",
        "    faiss.write_index(index, index_out_path)\n",
        "    print(f\"Saved index: {index.ntotal} vectors\")\n",
        "\n",
        "save_index_streaming('embedded_chunks_web.json', 'embedded_chunks_web.index')"
      ],
      "metadata": {
        "id": "fvsjRUhawt8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HFwn0mYmxT-C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}