{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f96a6964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from curses import raw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from typing import Optional, List, Mapping, Any, Union\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage,\n",
    "    SystemMessage,\n",
    "    ChatMessage,\n",
    ")\n",
    "\n",
    "from langchain_core.outputs import (\n",
    "    ChatGeneration,\n",
    "    ChatResult,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_llm(type: str = \"google\", cfg: dict = None):\n",
    "    if type == \"large_vnpt\":\n",
    "        return LargeLLM(**cfg)\n",
    "    if type == \"small_vnpt\":\n",
    "        return SmallLLM(**cfg)\n",
    "    if type == \"google\":\n",
    "        return ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            temperature=cfg.get(\"temperature\", 0.3) if cfg else 0.3,\n",
    "            google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "        )\n",
    "    \n",
    "\n",
    "\n",
    "# class LargeLLM(LLM):\n",
    "        \n",
    "\n",
    "#     def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "#         headers = {\n",
    "#             'Authorization': os.getenv(\"AUTHORIZATION_VNPT_LARGE\"),\n",
    "#             'Token-id': os.getenv(\"TOKEN_ID_VNPT_LARGE\"),\n",
    "#             'Token-key': os.getenv(\"TOKEN_KEY_VNPT_LARGE\"),\n",
    "#             'Content-Type': 'application/json',\n",
    "#         }\n",
    "\n",
    "#         json_data = {\n",
    "#             \"model\": \"vnptai_hackathon_large\",\n",
    "#             \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "#             \"temperature\": 1.0,\n",
    "#             \"top_p\": 1.0,\n",
    "#             \"top_k\": 20,\n",
    "#             \"n\": 1,\n",
    "#             \"max_completion_tokens\": 100,\n",
    "#         }\n",
    "\n",
    "#         response = requests.post(\n",
    "#             \"https://api.idg.vnpt.vn/data-service/vnptai-hackathon-large\",\n",
    "#             headers=headers,\n",
    "#             json=json_data\n",
    "#         )\n",
    "\n",
    "#         try:\n",
    "#             return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "#         except Exception:\n",
    "#             return str(response.json())\n",
    "\n",
    "#     @property\n",
    "#     def _identifying_params(self) -> Mapping[str, Any]:\n",
    "#         return {}\n",
    "\n",
    "#     @property\n",
    "#     def _llm_type(self) -> str:\n",
    "#         return \"vnpt_ai\"\n",
    "\n",
    "class SmallLLM(BaseChatModel):\n",
    "    \n",
    "    temperature: float = 0.3\n",
    "    top_p: float = 1.0\n",
    "    top_k: int = 20\n",
    "    max_tokens: int = 512\n",
    "    response_format: Optional[dict] = None\n",
    "    \n",
    "    def __init__(self, temperature: float = 0.3, top_p: float = 1.0, top_k: int = 20, max_tokens: int = 512, response_format: Optional[dict] = None) -> None:\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "        self.top_k = top_k\n",
    "        self.max_tokens = max_tokens\n",
    "        self.response_format = response_format\n",
    "\n",
    "\n",
    "    # ---- LangChain required properties ----\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"vnpt_ai\"\n",
    "\n",
    "    # ---- Core generate function ----\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "    ):\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {os.getenv('AUTHORIZATION_VNPT_SMALL')}\",\n",
    "            \"Token-id\": os.getenv(\"TOKEN_ID_VNPT_SMALL\"),\n",
    "            \"Token-key\": os.getenv(\"TOKEN_KEY_VNPT_SMALL\"),\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "        \n",
    "        # Convert LangChain messages -> VNPT format\n",
    "        payload_messages = []\n",
    "        for m in messages:\n",
    "            if isinstance(m, HumanMessage):\n",
    "                payload_messages.append({\"role\": \"user\", \"content\": m.content})\n",
    "            elif isinstance(m, AIMessage):\n",
    "                payload_messages.append({\"role\": \"assistant\", \"content\": m.content})\n",
    "            elif isinstance(m, SystemMessage):\n",
    "                payload_messages.append({\"role\": \"system\", \"content\": m.content})\n",
    "            elif isinstance(m, ChatMessage):\n",
    "                payload_messages.append({\"role\": m.role, \"content\": m.content})\n",
    "\n",
    "        json_data = {\n",
    "            \"model\": \"vnptai_hackathon_small\",\n",
    "            \"messages\": payload_messages,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"top_p\": self.top_p,\n",
    "            \"top_k\": self.top_k,\n",
    "            \"max_completion_tokens\": self.max_tokens,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            res = requests.post(\n",
    "                \"https://api.idg.vnpt.vn/data-service/v1/chat/completions/vnptai-hackathon-small\",\n",
    "                headers=headers,\n",
    "                json=json_data,\n",
    "                timeout=100\n",
    "            ).json()\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"VNPT API request error: {e}\")\n",
    "\n",
    "        # Debug nếu API trả về lỗi, không có 'choices'\n",
    "        if \"choices\" not in res:\n",
    "            raise ValueError(f\"VNPT API logical error response: {res}\")\n",
    "\n",
    "        text = res[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        return ChatResult(\n",
    "            generations=[ChatGeneration(message=AIMessage(content=text))]\n",
    "        )\n",
    "\n",
    "class LargeLLM(BaseChatModel):\n",
    "    \n",
    "    temperature: float = 0.3\n",
    "    top_p: float = 1.0\n",
    "    top_k: int = 20\n",
    "    max_tokens: int = 256\n",
    "    response_format: Optional[dict] = None\n",
    "    \n",
    "    def __init__(self, temperature: float = 0.3, top_p: float = 1.0, top_k: int = 20, max_tokens: int = 256, response_format=None) -> None:\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "        self.top_k = top_k\n",
    "        self.max_tokens = max_tokens\n",
    "        self.response_format = response_format\n",
    "        \n",
    "\n",
    "\n",
    "    # ---- LangChain required properties ----\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"vnpt_ai\"\n",
    "\n",
    "    # ---- Core generate function ----\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "    ):\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {os.getenv('AUTHORIZATION_VNPT_LARGE')}\",\n",
    "            \"Token-id\": os.getenv(\"TOKEN_ID_VNPT_LARGE\"),\n",
    "            \"Token-key\": os.getenv(\"TOKEN_KEY_VNPT_LARGE\"),\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "        \n",
    "        # Convert LangChain messages -> VNPT format\n",
    "        payload_messages = []\n",
    "        for m in messages:\n",
    "            if isinstance(m, HumanMessage):\n",
    "                payload_messages.append({\"role\": \"user\", \"content\": m.content})\n",
    "            elif isinstance(m, AIMessage):\n",
    "                payload_messages.append({\"role\": \"assistant\", \"content\": m.content})\n",
    "            elif isinstance(m, SystemMessage):\n",
    "                payload_messages.append({\"role\": \"system\", \"content\": m.content})\n",
    "            elif isinstance(m, ChatMessage):\n",
    "                payload_messages.append({\"role\": m.role, \"content\": m.content})\n",
    "\n",
    "        json_data = {\n",
    "            \"model\": \"vnptai_hackathon_large\",\n",
    "            \"messages\": payload_messages,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"top_p\": self.top_p,\n",
    "            \"top_k\": self.top_k,\n",
    "            \"max_completion_tokens\": self.max_tokens,\n",
    "            \"response_format\": self.response_format\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            res = requests.post(\n",
    "                \"https://api.idg.vnpt.vn/data-service/v1/chat/completions/vnptai-hackathon-large\",\n",
    "                headers=headers,\n",
    "                json=json_data,\n",
    "                timeout=100\n",
    "            ).json()\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"VNPT API request error: {e}\")\n",
    "\n",
    "        # Debug nếu API trả về lỗi, không có 'choices'\n",
    "        if \"choices\" not in res:\n",
    "            raise ValueError(f\"VNPT API logical error response: {res}\")\n",
    "\n",
    "        text = res[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        return ChatResult(\n",
    "            generations=[ChatGeneration(message=AIMessage(content=text))]\n",
    "        )\n",
    "        \n",
    "\n",
    "class Router:\n",
    "    def __init__(self, type_llm=\"large_vnpt\"):\n",
    "        self.llm = get_llm(type=type_llm, cfg={\"response_format\": {\"type\": \"json_object\"}})\n",
    "        self.prompt = self._get_prompt()\n",
    "        self.chain = self.prompt | self.llm\n",
    "        \n",
    "    def _get_prompt(self):\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                SystemMessage(content=ROUTER_SYSTEM_PROMPT),\n",
    "                HumanMessagePromptTemplate.from_template(ROUTER_HUMAN_PROMPT),\n",
    "            ]\n",
    "        )\n",
    "        return prompt\n",
    "        \n",
    "    \n",
    "    def route(self, question: str) -> dict:\n",
    "        len_words = count_word(question)\n",
    "        if len_words > 500:\n",
    "            return {\n",
    "                \"datasource\": \"Long_Text_Questions\",\n",
    "                \"reasoning\": f\"Văn bản có độ dài {len_words} từ, vượt quá 500 từ.\"\n",
    "            }\n",
    "        data = self.chain.invoke({\"question\": question})\n",
    "        raw = data.content.encode('utf-8')\n",
    "        return json.loads(raw)\n",
    "\n",
    "class VNPTAIEmbeddingClient:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"vnptai_hackathon_embedding\",\n",
    "        timeout: int = 100\n",
    "    ):\n",
    "        self.api_url = 'https://api.idg.vnpt.vn/data-service/vnptai-hackathon-embedding'\n",
    "        self.model = model\n",
    "        self.timeout = timeout\n",
    "\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {os.getenv('AUTHORIZATION_VNPT_EMBED')}\",\n",
    "            \"Token-id\": os.getenv('TOKEN_ID_VNPT_EMBED'),\n",
    "            \"Token-key\": os.getenv('TOKEN_KEY_VNPT_EMBED'),\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "\n",
    "    def embed(\n",
    "        self,\n",
    "        texts: Union[str, List[str]],\n",
    "        normalize: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        texts: string hoặc list[string]\n",
    "        return: embedding vector hoặc list[vector]\n",
    "        \"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "            single = True\n",
    "        else:\n",
    "            single = False\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"input\": texts,\n",
    "            \"encoding_format\": \"float\"\n",
    "        }\n",
    "\n",
    "        response = requests.post(\n",
    "            self.api_url,\n",
    "            headers=self.headers,\n",
    "            json=payload,\n",
    "            timeout=self.timeout\n",
    "        )\n",
    "\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        embeddings = [item[\"embedding\"] for item in data[\"data\"]]\n",
    "\n",
    "        if normalize:\n",
    "            embeddings = [self._normalize(vec) for vec in embeddings]\n",
    "\n",
    "        return embeddings[0] if single else embeddings\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize(vec):\n",
    "        norm = sum(v * v for v in vec) ** 0.5\n",
    "        return [v / norm for v in vec] if norm > 0 else vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7107f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "corpus = json.load(open('./corpus/chunks_web.json'))\n",
    "\n",
    "embedder = VNPTAIEmbeddingClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f1f0408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 22677/37359 [1:01:50<5:25:40,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retried 2 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 29790/37359 [1:22:45<37:43:44, 17.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retried 2 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 34788/37359 [1:35:11<9:58:28, 13.97s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retried 2 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37359/37359 [1:40:48<00:00,  6.18it/s]  \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "NUM_RETRIES = 50\n",
    "for item in tqdm(corpus):\n",
    "    for turn_id in range(NUM_RETRIES):\n",
    "        try: \n",
    "            item['embeddings'] = embedder.embed(item['content'])\n",
    "            if turn_id > 0: print(f'retried {turn_id + 1} times')\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if turn_id < NUM_RETRIES - 1: time.sleep(1.0); continue\n",
    "            print(\"Max retries exceeded\")\n",
    "            raise e\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ea5baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(corpus, open('./corpus/embedded_chunks_web.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69953af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
